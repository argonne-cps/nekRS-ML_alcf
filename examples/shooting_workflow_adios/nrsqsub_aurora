#!/bin/bash
set -e

#--------------------------------------
: ${QUEUE:="prod"}
: ${NEKRS_GPU_MPI:=0}
: ${NEKRS_BACKEND:="dpcpp"}
: ${RANKS_FOR_BUILD:=12}
: ${SIM_RANKS_PER_NODE:=6}
: ${TRAIN_RANKS_PER_NODE:=6}
: ${DEPLOYMENT:="colocated"}
: ${SIM_NODES:=1}
: ${TRAIN_NODES:=1}
: ${SIM_CPU_BIND_LIST:="1:8:16:24:32:40"}
: ${TRAIN_CPU_BIND_LIST:="53:60:68:76:84:92"}
: ${INFERENCE_CPU_BIND_LIST:="1:8:16:24:32:40:53:60:68:76:84:92"}
: ${OCCA_DPCPP_COMPILER_FLAGS:="-O3 -fsycl -fsycl-targets=intel_gpu_pvc -ftarget-register-alloc-mode=pvc:auto -fma"}
: ${ONEAPI_SDK:=""}
: ${FRAMEWORKS_MODULE:="frameworks"}
: ${VENV_PATH:=""}
#--------------------------------------

#--------------------------------------
# Copy the .par file
cp ${1}.par.safe ${1}.par

#--------------------------------------
# Update the .box file to increase the mesh size linearly with the number of nodes
SIM_PROCS=$(( SIM_NODES * SIM_RANKS_PER_NODE ))
#NX=$(( 18*SIM_NODES ))
#NZ=$(( 10*SIM_NODES ))
NX=$(( 13 ))
NZ=$(( 2*SIM_PROCS ))
if [[ "$SIM_NODES" -gt 1 ]]; then
    LZ=$SIM_NODES
else
    LZ=1
fi
cp ${1}_aurora.box ${1}.box
sed -i "s/NX/${NX}/g" ${1}.box
sed -i "s/NZ/${NZ}/g" ${1}.box
sed -i "s/LZ/${LZ}/g" ${1}.box

#--------------------------------------
# Run genbox from Nek5000 to generate the .re2 mesh file
/flare/datascience/balin/Nek/Nek5000/bin/genbox ${1}.box ${1}

#--------------------------------------
# Setup the case
source $NEKRS_HOME/bin/nrsqsub_utils
setup $# 1
chk_case $TOTAL_RANKS

#--------------------------------------
# Generate the affinity scripts
SAFF_FILE=${WORK_DIR}/affinity_nrs.sh
TAFF_FILE=${WORK_DIR}/affinity_ml.sh
if [ ${ZE_FLAT_DEVICE_HIERARCHY} == "COMPOSITE" ] || [[ -z "${ZE_FLAT_DEVICE_HIERARCHY}" ]]; then
  echo "#!/bin/bash" > $SAFF_FILE
  echo "num_gpus=\$1" >> $SAFF_FILE
  echo "shift" >> $SAFF_FILE
  echo "num_tiles=2" >> $SAFF_FILE
  echo "gpu_id=\$(((PALS_LOCAL_RANKID / \${num_tiles}) % \${num_gpus}))" >> $SAFF_FILE
  echo "tile_id=\$((PALS_LOCAL_RANKID % \${num_tiles}))" >> $SAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id.\$tile_id" >> $SAFF_FILE
  echo "exec \"\$@\"" >> $SAFF_FILE

  echo "#!/bin/bash" > $TAFF_FILE
  echo "num_gpus=\$1" >> $TAFF_FILE
  echo "offset=\$2" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "num_tiles=2" >> $TAFF_FILE
  echo "gpu_id=\$(((PALS_LOCAL_RANKID / \${num_tiles}) % \${num_gpus} + (offset / num_tiles) ))" >> $TAFF_FILE
  echo "tile_id=\$((PALS_LOCAL_RANKID % \${num_tiles}))" >> $TAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id.\$tile_id" >> $TAFF_FILE
  echo "exec \"\$@\"" >> $TAFF_FILE
elif [ ${ZE_FLAT_DEVICE_HIERARCHY} == "FLAT" ]; then
  echo "#!/bin/bash" > $SAFF_FILE
  echo "num_gpus=\$1" >> $SAFF_FILE
  echo "shift" >> $SAFF_FILE
  echo "gpu_id=\$((PALS_LOCAL_RANKID % \${num_gpus} ))" >> $SAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id" >> $SAFF_FILE
  echo "exec \"\$@\"" >> $SAFF_FILE

  echo "#!/bin/bash" > $TAFF_FILE
  echo "num_gpus=\$1" >> $TAFF_FILE
  echo "offset=\$2" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "gpu_id=\$((PALS_LOCAL_RANKID % \${num_gpus} + \${offset} ))" >> $TAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id" >> $TAFF_FILE
  echo "exec \"\$@\"" >> $TAFF_FILE
fi
chmod u+x $SAFF_FILE $TAFF_FILE

#--------------------------------------
# Generate the workflow config script
CFILE=${WORK_DIR}/config.yaml
echo "# Workflow config" > $CFILE
echo "scheduler: pbs" >> $CFILE
echo "deployment: \"${DEPLOYMENT}\"" >> $CFILE
echo "" >> $CFILE
echo "# Run config" >> $CFILE
echo "run_args:" >> $CFILE
echo "    nodes: ${qnodes}" >> $CFILE
if [ ${DEPLOYMENT} == "colocated"  ]; then
  SIM_NODES=$nodes
  TRAIN_NODES=$nodes
  SIM_PROCS=$(( SIM_NODES * SIM_RANKS_PER_NODE ))
  TRAIN_PROCS=$(( TRAIN_NODES * TRAIN_RANKS_PER_NODE ))
  echo "    sim_nodes: ${SIM_NODES}" >> $CFILE
  echo "    ml_nodes: ${SIM_NODES}" >> $CFILE
  echo "    simprocs: ${SIM_PROCS}" >> $CFILE
  echo "    simprocs_pn: ${SIM_RANKS_PER_NODE}" >> $CFILE
  echo "    mlprocs: ${TRAIN_PROCS}" >> $CFILE
  echo "    mlprocs_pn: ${TRAIN_RANKS_PER_NODE}" >> $CFILE
  echo "    sim_cpu_bind: \"list:${SIM_CPU_BIND_LIST}\"" >> $CFILE
  echo "    ml_cpu_bind: \"list:${TRAIN_CPU_BIND_LIST}\"" >> $CFILE
elif [ ${DEPLOYMENT} == "clustered"  ]; then
  SIM_PROCS=$(( SIM_NODES * SIM_RANKS_PER_NODE ))
  TRAIN_PROCS=$(( TRAIN_NODES * TRAIN_RANKS_PER_NODE ))
  echo "    sim_nodes: ${SIM_NODES}" >> $CFILE
  echo "    ml_nodes: ${TRAIN_NODES}" >> $CFILE
  echo "    simprocs: ${SIM_PROCS}" >> $CFILE
  echo "    simprocs_pn: ${SIM_RANKS_PER_NODE}" >> $CFILE
  echo "    mlprocs: ${TRAIN_PROCS}" >> $CFILE
  echo "    mlprocs_pn: ${TRAIN_RANKS_PER_NODE}" >> $CFILE
  echo "    sim_cpu_bind: \"list:${SIM_CPU_BIND_LIST}\"" >> $CFILE
  echo "    ml_cpu_bind: \"list:${SIM_CPU_BIND_LIST}\"" >> $CFILE
fi
echo "" >> $CFILE
echo "# Simulation config" >> $CFILE
echo "sim:" >> $CFILE
echo "    executable: \"${NEKRS_HOME}/bin/nekrs\"" >> $CFILE
echo "    arguments: \"--setup ${case}.par --backend ${NEKRS_BACKEND} --device-id 0\"" >> $CFILE
echo "    affinity: \"${SAFF_FILE}\"" >> $CFILE
echo "" >> $CFILE
echo "# Trainer config" >> $CFILE
echo "train:" >> $CFILE
echo "    executable: \"${NEKRS_HOME}/3rd_party/gnn/main.py\"" >> $CFILE
#echo "    affinity: \"./${TAFF_FILE}\"" >> $CFILE
echo "    affinity: \"\"" >> $CFILE
if [ ${DEPLOYMENT} == "colocated"  ]; then
  echo "    arguments: \"device_skip=${SIM_RANKS_PER_NODE} backend=ccl halo_swap_mode=all_to_all_opt online=True client.backend=adios client.adios_transport=RDMA consistency=True time_dependency=time_dependent verbose=True\"" >> $CFILE
elif [ ${DEPLOYMENT} == "clustered"  ]; then
  echo "    arguments: \"backend=ccl halo_swap_mode=all_to_all_opt online=True client.backend=adios consistency=True time_dependency=time_dependent verbose=True\"" >> $CFILE
fi
echo "" >> $CFILE
echo "# Inference config" >> $CFILE
echo "inference:" >> $CFILE
echo "    executable: \"${NEKRS_HOME}/3rd_party/gnn/inference.py\"" >> $CFILE
#echo "    affinity: \"./${TAFF_FILE}\"" >> $CFILE
echo "    affinity: \"\"" >> $CFILE
echo "    arguments: \"model_task=inference rollout_steps=100 backend=ccl halo_swap_mode=all_to_all_opt online=True client.backend=adios consistency=True time_dependency=time_dependent verbose=True\"" >> $CFILE

#--------------------------------------
# Update the .par file with the ML options
PFILE=${case}.par
echo "" >> $PFILE
echo "[ML]" >> $PFILE
echo "adiosEngine = SST" >> $PFILE
echo "adiosTransport = RDMA" >> $PFILE
echo "adiosStream = async" >> $PFILE

#--------------------------------------
# Generate the submit script
SFILE=submit_nekRSML.sh
echo "#!/bin/bash -l" > $SFILE

echo "#PBS -S /bin/bash" >>$SFILE
echo "#PBS -N nekRS_turbChannel" >>$SFILE
echo "#PBS -l select=4" >>$SFILE
echo "#PBS -l walltime=1:00:00" >>$SFILE
echo "#PBS -l filesystems=home:flare" >>$SFILE
echo "#PBS -A datascience" >>$SFILE
echo "#PBS -q debug-scaling" >>$SFILE
echo "#PBS -k doe" >>$SFILE
echo "#PBS -j oe" >>$SFILE

echo -e "\ncd \$PBS_O_WORKDIR" >> $SFILE
echo "export TZ='/usr/share/zoneinfo/US/Central'" >> $SFILE

echo -e "\necho Jobid: \$PBS_JOBID" >>$SFILE
echo "echo Running on host \`hostname\`" >>$SFILE
echo "echo Running on nodes \`cat \$PBS_NODEFILE\`" >>$SFILE

echo "module load ${FRAMEWORKS_MODULE}" >> $SFILE
echo "source ${VENV_PATH}" >> $SFILE
echo "module list" >> $SFILE

echo -e "\nexport NEKRS_HOME=$NEKRS_HOME" >>$SFILE
echo "export NEKRS_GPU_MPI=$NEKRS_GPU_MPI" >>$SFILE
#echo "export MPICH_GPU_SUPPORT_ENABLED=$NEKRS_GPU_MPI" >> $SFILE

echo "export OCCA_DPCPP_COMPILER_FLAGS=\"$OCCA_DPCPP_COMPILER_FLAGS\"" >> $SFILE

# Cray MPI libfabric defaults
echo -e "\n# MPI vars" >>$SFILE
echo "unset MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE" >> $SFILE
echo "unset MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE" >> $SFILE
echo "unset MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE" >> $SFILE
echo "export FI_CXI_RDZV_THRESHOLD=16384" >> $SFILE
echo "export FI_CXI_RDZV_EAGER_SIZE=2048" >> $SFILE
echo "export FI_CXI_DEFAULT_CQ_SIZE=131072" >> $SFILE
echo "export FI_CXI_DEFAULT_TX_SIZE=1024" >> $SFILE
echo "export FI_CXI_OFLOW_BUF_SIZE=12582912" >> $SFILE
echo "export FI_CXI_OFLOW_BUF_COUNT=3" >> $SFILE
echo "export FI_CXI_REQ_BUF_MIN_POSTED=6" >> $SFILE
echo "export FI_CXI_REQ_BUF_SIZE=12582912" >> $SFILE
echo "export FI_MR_CACHE_MAX_SIZE=-1" >> $SFILE
echo "export FI_MR_CACHE_MAX_COUNT=524288" >> $SFILE
echo "export FI_CXI_REQ_BUF_MAX_CACHED=0" >> $SFILE
echo "export FI_CXI_REQ_BUF_MIN_POSTED=6" >> $SFILE
# echo "export FI_CXI_RX_MATCH_MODE=hardware" >> $SFILE
echo "export FI_CXI_RX_MATCH_MODE=hybrid" >> $SFILE # required by parRSB

echo -e "\n# oneCCL vars" >>$SFILE
echo "export CCL_PROCESS_LAUNCHER=pmix" >>$SFILE
echo "export CCL_ATL_TRANSPORT=mpi" >>$SFILE
echo "export CCL_ALLREDUCE_SCALEOUT=direct:0-1048576;rabenseifner:1048577-max" >>$SFILE
echo "export CCL_BCAST=double_tree" >>$SFILE
echo "export CCL_KVS_MODE=mpi" >>$SFILE
echo "export CCL_CONFIGURATION_PATH=\"\"" >>$SFILE
echo "export CCL_CONFIGURATION=cpu_gpu_dpcpp" >>$SFILE
echo "export CCL_KVS_CONNECTION_TIMEOUT=600" >>$SFILE
echo "export CCL_ZE_CACHE_OPEN_IPC_HANDLES_THRESHOLD=1024" >>$SFILE
echo "export CCL_KVS_USE_MPI_RANKS=1" >>$SFILE
echo "export CCL_ALLTOALLV_MONOLITHIC_KERNEL=0" >> $SFILE

echo -e "\n# ADIOS2 vars" >>$SFILE
echo "export PYTHONPATH=\$PYTHONPATH:${NEKRS_HOME}/lib/python3.10/site-packages" >> $SFILE
echo "#export SstVerbose=1" >> $SFILE
echo "export OMP_PROC_BIND=spread" >> $SFILE
echo "export OMP_PLACES=threads" >> $SFILE
echo "if ls *.sst 1> /dev/null 2>&1" >> $SFILE
echo "then" >> $SFILE 
echo "    echo Cleaning up old .sst files" >> $SFILE
echo "    rm *.sst" >> $SFILE
echo "fi" >> $SFILE
echo "if ls *.bp 1> /dev/null 2>&1" >> $SFILE
echo "then" >> $SFILE
echo "    echo Cleaning up old .bp files" >> $SFILE
echo "    rm -r ./*.bp" >> $SFILE
echo "fi" >> $SFILE

echo -e "\n# precompilation" >>$SFILE
echo "if [ ! -d .cache ]; then" >> $RFILE
echo "  echo Cleaning up the cache" >> $RFILE
echo "  rm -r .cache" >> $RFILE
echo "fi" >> $RFILE
CMD_build="mpiexec -n ${RANKS_FOR_BUILD} -ppn ${RANKS_FOR_BUILD} --cpu-bind list:1:8:16:24:32:40:53:60:68:76:84:92 -- ./${SAFF_FILE} ${RANKS_FOR_BUILD} $bin --setup ${case} --backend ${NEKRS_BACKEND} --device-id 0 $extra_args --build-only ${RANKS_FOR_BUILD}"
add_build_CMD "$SFILE" "$CMD_build" "$TOTAL_RANKS"

echo -e "\n# actual run" >>$SFILE
echo "python driver.py" >> $SFILE
chmod u+x $SFILE

